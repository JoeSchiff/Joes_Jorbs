
errors:
pickle.dump() start giving "maximum recursion depth exceeded while calling a Python object +



Todo:
replace with with async with when necessary + might fix pickle recursion error -
static resp.close and session.close() - already closed with context managers

refactor
  any func with instance as arg should be method?
  clean return unn?
  
separate functions
  jbw count and write file +
  robots and proceed
  looper
  pw reqer
    combine success and fail conditions for pw and static
  pagination +
  check net +
  post scrape

logging module
  always print task id + when available
  must include stderr +
  determine log levels +

handle timeouts uniformly +
  check async timeout again
  child frames

respect robots.txt
  store rp files or make async
  after success req: update timestamp and domain count +
    after any req? not just success?
  call proceed_f on initial urls +
  default useragent
  use 2 useragents to see difference?
  pass rp into working_o?
  implement domain rate limiter +
      include timestamp of last req for that domain +
        dict of domains: obj as value? +
        save and recover +
        global default rate limiter also
    why class? use attributes not dict +
    use self in methods +

auto blacklist +
    blacklist is dup urls +
    must be exact match, not domain. ex: 5il.co != 5il.co/page3
domain-wide jbw conf. Once high conf is established, ignore low conf links for that domain
auto update dbs urls by sorting jbw conf from scraper
  this wont get new orgs, only new em urls
cleanup before release:
  whitespace
  unn comments

pagination class never works. test 'next' or '>' test +
set playwright user agent?
all (working_o) funcs as class methods
different working_o classes: -
    before req: add_to_queue
    during: add_html, vis_soup_f, pw_req, static_req, child_frame
    after: fallback_success, check_red, check_vis_text, write_results, crawler
    errors? anytime?
    before and after are same. loop is a circle
use asyncio.Event() instead of blocking: all_done_d,l pw_pause, asyncio.sleep
net::ERR_NAME_NOT_RESOLVED should also be 404
call crawler from add_html method?
all pause for static if bash ping fails
include error result in working_o?
need checked lock? - dup ensures a url is processed only once, therefore one task at a time
cml as a class?
looper should be only handle which requester to use. main before and after?
aiofiles for async file ops
only use async when neccesary:
    reqs: looper
    IO: write results, save prog, prant?
dont have sync and async versions of dup_check and checked_entry +
why use methods?
check for malformed urls? >>> r=urlparse('http:/joesjorbs.com') >>> if r.scheme and r.netloc: print(99)
rewrite crawler
prog count higher than total
remove multiorg? results.py doesnt show duplicate urls
all pause +
  how check? start with pw then fallback to bash ping? pw > ping url > ping ip
      url not ip to test dns?
redundant error 7
  final or try with next reqer?
  pw gets html and vis
replacement for pkill -9 node/firefox
discard head elem from html. page.inner_html('body')
  resp.text vs page.inner_html vs page.text_content vs page.content() etc
establish new jj_error numbers and letters
add redirect history to checked pages. can be done with aiohttp, cant find for pw
state what each func returns at top of func
parent url may not be in errorlog on first error - remove code

update portals
  allow multiple em urls for each org in db?
      use urls from only same org. ex: dont use county url for town. diff orgs
      error on any em url in list would call for fallback. implications?
      city oswego, orleans, st lawco


properly track skipped pages?
put skipped pages into cml? might give dups
"application" as bunkword?
use empty list placeholder for jj_final_error and fallback_success in errorlog?
errorlog as json?
sort results to either regular dir or empty vis text dir (for debugging)
fallback to domain after homeurl fallback?
mark all nonlogged errors with underscore or remove try block
use both a domain limiter and a limiter based on full url (except query)?
create unique codes for all skips. print and mark in cml
improve bunkwords: mark all skips in outcome. dont use list comprehension? print offending bunkword and context



winter update project:
  which scraper to use?
  use double quotes. watch out for replacing possesive apostrophes
  update em urls and home urls
  search for new orgs
  verify coords?
document which orgs use a centralized service and exclude or include them from jj search. ie: applitrack/caboces, applitrack/penfield, etc
dups in db. probably causes the dups found in cml? solved with multi org d?


results.py to do:
move zips out of file
reconsider skipping geodesic with high max_dist
remove zip_form? display purposes only. keep
show jj_error num in tooltip on error tabs?
improve code comments
optimise: two separate sections. one with geolimter function
reword error pages to discourage refreshing?
sort errors by alpha?
sort results by jbw conf or fuzzy match percent? user specified -
percent decode urls
wraparound text for mobile?
no error will be displayed if failed to read vis text file


index.html to do:
dup zip codes
fix indents
obfuscate -
improve code comments
zip_dict one entry per line?
hide modal after back button without refresh - difficult
show progress on modal - difficult
new modal over old for progress?
create favicon


to do later:
search PDFs from webpages
only firefox can detect pdf cleanly
run scraper as cron job
put all errors in add_errorurls_f. eg: __error. or use jj error 9 catch all for __errors?
jbws back to count but limit to x occurrences?
decompose nav tags?
content of script tags not decomposing because Splash evaluates scripts. So there is no script tag header or footer for BS to read: https://recruiting.ultipro.com/BRY1002BSC/JobBoard/6b838b9a-cd2b-436a-903b-0de7b6e17b4f/?q=&o=postedDateDesc
max crawl depth 3? either high or low conf jbws in tags? -
remove non printable characters from result text? -
phase out blacklist
weighted jbws
upgrade server to 22.04
charter schools http://www.p12.nysed.gov/psc/csdirectory/CSLaunchPage.html



false positives: include keyword and date
https://www.herkimer.edu/about/employment/
https://hr.cornell.edu/jobs       librarian 1/20
https://www.newvisions.org/pages/media-centers-for-the-21st-century
https://www.tbafcs.org/Page/1444  nurse 2/20 dropdown


All fallback types: static fb, portal to homepage fb, include_old fb


Concerns:

Dup checker: 
remove after ampersand in query?
remove fragments and trailing slash. yes
case sensitivity. yes

High conf: exclude good low conf links
https://www.cityofnewburgh-ny.gov/civil-service = upcoming exams
http://www.albanycounty.com/Government/Departments/DepartmentofCivilService.aspx = exam announcement
have separate high conf jbw lists?
accept links with only high conf job words?

Bunkwords: search entire element or just contents?
must search url to exclude .pdf, etc

Decompose: drop down menus?
dont decompose menus for anchor tag search +

No space between elements' content in results
caused by converting from soup to soup.text
eg: Corporation Counsel</option><option>Downtown Parking Improvement
produces this: developmentcorporation counseldowntown parking improvement planengineeringethics
this shouldn't matter because a keyword probably won't span accross multiple elements

use urllib or manual replace to percent encode urls?
url_path = workingurl.replace('/', '%2F')  # or
url_path = parse.quote(workingurl, safe=':')

 


 
