
errors:
progress higher than total + homepage fallback
progress lower than total at end

remove "Cant append error to errorlog"?

remove unn locks +

Todo:
recover q loses all working urls. keep track of current urls being requested and combine with q prior to writing to file?
rewrite errorlog. with logging?
    class inst to be more descriptive?
    init with empty lists?
    put all minor errors into new jj error
    cml_text = '{\n'

use pw req obj for ping
    ping as child class of pw?

refactor
    pw and aio sessions in their req class
    create locks in method?
    break up clear_brows
    rename working_o to scrap? pre_req and post?
    curernt:
        add_to_queue, fallback_success, check_red, count_jbws, write_results
    funcs can be methods:
        choose_requester+, req_success, add_errorurls, final_error, homepage_fallback+, get_pagination+, get_links+, crawler+
    back_in_q_b as sep func?
    pass q, err log etc or make working_o class var?
    clean return unn?  useful for adding new workingo to queue
    results.py and index


results.py to do:
    test {e<=0}
    remove from import
    dict or class for res_list
    reg_res = None
    why browser in each results file?
    move zips out of file +
    reconsider skipping geodesic with high max_dist
    remove zip_form? display purposes only. keep
        why send coords from form?
    show jj_error num in tooltip on error tabs?
    improve code comments
    optimise: two separate sections. one with geolimter function
    reword error pages to discourage refreshing?
    sort errors by alpha?
    sort results by jbw conf or fuzzy match percent? user specified -
    percent decode urls
    wraparound text for mobile?
    no error will be displayed if failed to read vis text file

remove?
    multiorg: results.py doesnt show duplicate urls
    CML
    clear_brows, brow_l: only 1 brow needed since pw stable
    bash_ping, restart_nic

logging module
    new id is assigned when using static reqs

handle timeouts uniformly +
  asyncio.TimeoutError not possible without wait_for?
  check async timeout again
  child frames

respect robots.txt
  does rp file have multiple dates? so how determine exiration?
    when are new entries created?
        which domains are scraped never change therfore only dates change? or new entries are made as scraper runs?
  store rp files or make async
  after success req: update timestamp and domain count +
    after any req? not just success?
  call proceed_f on initial urls +
  default useragent
  use 2 useragents to see difference?
  pass rp into working_o?
  implement domain rate limiter +
      include timestamp of last req for that domain +
        dict of domains: obj as value? +
        save and recover +
        global default rate limiter also
    why class? use attributes not dict +
    use self in methods +

auto blacklist
    static bl is unn
    blacklist is dup urls +
    must be exact match, not domain. ex: 5il.co != 5il.co/page3
    
domain-wide jbw conf. Once high conf is established, ignore low conf links for that domain
auto update dbs urls by sorting jbw conf from scraper
  this wont get new orgs, only new em urls
cleanup before release:
  whitespace
  unn comments

pagination class never works. test 'next' or '>' test +
set playwright user agent?
use asyncio.Event() instead of blocking: all_done_d,l pw_pause, asyncio.sleep
net::ERR_NAME_NOT_RESOLVED should also be 404
include error result in working_o?
need checked lock? - dup ensures a url is processed only once, therefore one task at a time
check for malformed urls? >>> r=urlparse('http:/joesjorbs.com') >>> if r.scheme and r.netloc: print(99)
redundant error 7
  final or try with next reqer?
discard head elem from html. page.inner_html('body')
  resp.text vs page.inner_html vs page.text_content vs page.content() etc
add redirect history to checked pages. can be done with aiohttp, cant find for pw
parent url may not be in errorlog on first error - remove code

update portals
  allow multiple em urls for each org in db?
      use urls from only same org. ex: dont use county url for town. diff orgs
      error on any em url in list would call for fallback. implications?
      city oswego, orleans, st lawco


properly track skipped pages?
put skipped pages into cml? might give dups
"application" as bunkword?
use empty list placeholder for jj_final_error and fallback_success in errorlog?
errorlog as json?
sort results to either regular dir or empty vis text dir (for debugging)
fallback to domain after homeurl fallback?
mark all nonlogged errors with underscore or remove try block
use both a domain limiter and a limiter based on full url (except query)?
create unique codes for all skips. print and mark in cml
improve bunkwords: mark all skips in outcome. dont use list comprehension? print offending bunkword and context



winter update project:
  which scraper to use?
  use double quotes. watch out for replacing possesive apostrophes
  update em urls and home urls
  search for new orgs
  verify coords?
document which orgs use a centralized service and exclude or include them from jj search. ie: applitrack/caboces, applitrack/penfield, etc
dups in db. probably causes the dups found in cml? solved with multi org d?





index.html to do:
dup zip codes
fix indents
obfuscate -
improve code comments
zip_dict one entry per line?
hide modal after back button without refresh - difficult
show progress on modal - difficult
new modal over old for progress?
create favicon


to do later:
search PDFs from webpages
only firefox can detect pdf cleanly
run scraper as cron job
put all errors in add_errorurls_f. eg: __error. or use jj error 9 catch all for __errors?
jbws back to count but limit to x occurrences?
decompose nav tags?
content of script tags not decomposing because Splash evaluates scripts. So there is no script tag header or footer for BS to read: https://recruiting.ultipro.com/BRY1002BSC/JobBoard/6b838b9a-cd2b-436a-903b-0de7b6e17b4f/?q=&o=postedDateDesc
max crawl depth 3? either high or low conf jbws in tags? -
remove non printable characters from result text? -
weighted jbws
upgrade server to 22.04
charter schools http://www.p12.nysed.gov/psc/csdirectory/CSLaunchPage.html



false positives: include keyword and date
https://www.herkimer.edu/about/employment/
https://hr.cornell.edu/jobs       librarian 1/20
https://www.newvisions.org/pages/media-centers-for-the-21st-century
https://www.tbafcs.org/Page/1444  nurse 2/20 dropdown


All fallback types: static fb, portal to homepage fb, include_old fb


Concerns:

Dup checker: 
remove after ampersand in query?
remove fragments and trailing slash. yes
case sensitivity. yes

High conf: exclude good low conf links
https://www.cityofnewburgh-ny.gov/civil-service = upcoming exams
http://www.albanycounty.com/Government/Departments/DepartmentofCivilService.aspx = exam announcement
have separate high conf jbw lists?
accept links with only high conf job words?

Bunkwords: search entire element or just contents?
must search url to exclude .pdf, etc

Decompose: drop down menus?
dont decompose menus for anchor tag search +

No space between elements' content in results
caused by converting from soup to soup.text
eg: Corporation Counsel</option><option>Downtown Parking Improvement
produces this: developmentcorporation counseldowntown parking improvement planengineeringethics
this shouldn't matter because a keyword probably won't span accross multiple elements

use urllib or manual replace to percent encode urls?
url_path = workingurl.replace('/', '%2F')  # or
url_path = parse.quote(workingurl, safe=':')

 


 
